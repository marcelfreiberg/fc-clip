# python train_net.py --config-file configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_lars_coco.yaml --num-gpus 1 --eval-only

_BASE_: ../maskformer2_R50_bs16_50ep.yaml

MODEL:
  WEIGHTS: "/data/mfreiberg/weights/fcclip/fcclip_cocopan.pth"
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 11
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 50
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TEST: ("lars_coco_val_panoptic",)

OUTPUT_DIR: ./output